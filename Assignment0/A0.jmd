---
title : Assignment 0
author : Jiaqi Sun (#998181747)
options:
  eval: true #Set this to true if you'd like to evaluate the code in this document
---

# Probability

## Variance and Covariance
Let $X$ and $Y$ be two continuous, independent random variables.

1. [3pts] Starting from the definition of independence, show that the independence of $X$ and $Y$ implies that their covariance is $0$.

Answer:
Assuming $X$ and $Y$ are discrete random variables with probability mass function $P(X,Y)$.
By definition, if $X$ and $Y$ are independent, then $\mathbb{P}(X,Y) = \mathbb{P}(X) * \mathbb{P}(Y)$.
Then,
$$
\begin{align}
\mathbb{E}(XY) &= \sum_{X}\sum_{Y} XY * P(X,Y)\\
&=\sum_{X}\sum_{Y} XY * P(X)P(Y)\\
&=\sum_{X} XP(X) * \sum_{Y} YP(Y)\\
&=\mathbb{E}(X) * \mathbb{E}(Y)
\end{align}
$$
Since, covariance of $X$ and $Y$ is defined to be $\text{cov}(XY) = \mathbb{E}(XY) - \mathbb{E}(X) * \mathbb{E}(Y)$.
Thus, $\text{cov}(XY) = 0$. Similar approach for continuous random varaibles.

2. [3pts] For a scalar constant $a$, show the following two properties starting from the definition of expectation:

$$
\begin{align}
\mathbb{E}(X+aY) &= \mathbb{E}(X) + a\mathbb{E}(Y)\\
\text{var}(X + aY) &= \text{var}(X) + a^2 \text{var}(Y)
\end{align}
$$

Answer:
Assuming $X$ and $Y$ are discrete random variables with probability mass function $P(X,Y)$.
Expectaion of $X$ is defined to be $\mathbb{E}=\sum_{X} XP(X)$.
Thus,
$$
\begin{align}
\mathbb{E}(X+aY) &= \sum_{X}\sum_{Y} (X+aY)P(X,Y)\\
&= \sum_{X}\sum_{Y} (XP(X,Y) + aYP(X,Y))\\
&= \sum_{X}\sum_{Y} XP(X,Y) + \sum_{X}\sum_{Y} aYP(X,Y)\\
&= \sum_{X} XP(X) + \sum_{Y} aYP(Y)\\
&= \mathbb{E}(X) + a\mathbb{E}(Y)
\end{align}
$$

By definition, variance of $X$ is $\text{var}(X) = \mathbb{E}(X - \mathbb{E}(X))^2 = \mathbb{E}(X^2) - (\mathbb{E}(X))^2$.
Assuming $X$ and $Y$ are independent, i.e. $\text{cov}(XY) = \mathbb{E}(XY) - \mathbb{E}(X) * \mathbb{E}(Y) = 0$.
Thus,
$$
\begin{align}
\text{var}(X + aY) &= \mathbb{E}((X + aY)^2) - (\mathbb{E}(X + aY))^2\\
&= \mathbb{E}(X^2 + 2aXY + a^2Y^2) - (\mathbb{E}(X)^2 + 2a\mathbb{E}(X)\mathbb{E}(Y) + a^2\mathbb{E}(Y)^2)\\
&= [\mathbb{E}(X^2) - (\mathbb{E}(X))^2] + a^2[\mathbb{E}(Y^2) - (\mathbb{E}(Y))^2] + 2a(\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y))\\
&= \text{var}(X) + a^2 \text{var}(Y)
\end{align}
$$
Similar approach for continuous random varaibles.

## 1D Gaussian Densities

1. [1pts] Can a probability density function (pdf) ever take values greater than $1$?

Answer:
Yes. It is possible that pdf is greater than $1$, e.g. $f(x) = e^x$.
If $f(x)$ is a probability density function (pdf) of a continuous random variable $X$, then it satisfies following conditions:
*  $f(x) > 0\quad  \forall x$.
*  $\int_{-\infty}^{\infty} f(x) dx = 1$.
Thus, $f(x)$ can be greater then $1$ as long as it is intergrated to be 1 over defined interval.

2.  Let $X$ be a univariate random variable distributed according to a Gaussian distribution with mean $\mu$ and variance $\sigma^2$.

* [[1pts]] Write the expression for the pdf:

Answer:
Let $X$ be a random variable following a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. Its pdf writes as
$$
\begin{align}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{- \frac{(x - \mu)^2}{2\sigma ^2}}
\end{align}
$$

* [[2pts]] Write the code for the function that computes the pdf at $x$ with default values $\mu=0$ and $\sigma = \sqrt{0.01}$:

Answer:

```julia;
    function gaussian_pdf(x; mean=0., variance=0.01)
      return (1/sqrt(2*pi*variance)) * exp((-(x-mean)^2)/(2*variance))
      # implement pdf at x
    end
```

Test your implementation against a standard implementation from a library:

```julia
    using Test
    # Test answers
    using Distributions: pdf, Normal # Note Normal uses N(mean, stddev) for parameters
    @testset "Implementation of Gaussian pdf" begin
      x = randn()
      @test gaussian_pdf(x) ≈ pdf.(Normal(0.,sqrt(0.01)),x)
      # ≈ is syntax sugar for isapprox, typed with `\approx <TAB>`
      # or use the full function, like below
      @test isapprox(gaussian_pdf(x,mean=10., variance=1) , pdf.(Normal(10., sqrt(1)),x))
    end;
```

3. [1pts] What is the value of the pdf at $x=0$? What is probability that $x=0$ (hint: is this the same as the pdf? Briefly explain your answer.)

Answer:
pdf at $x=0$ is 0, while probability that $x=0$ could be any valune in $[0,1]$.
The reason is that pdf is for continuous random variables. To calculate the probability of a continuous random variable $X$ falling in certain intervals, we need to take a intergral over the intervals.
Thus, if pdf taking on any particular value instead of a range, it will have inifinite number of possible values to begin with. To ensure the sum of all the possibilities is 1, the pdf has to be 0.
Probability, on the other hand, is more for discrete random variable, working similar as probability mass function (pmf) where we can clearly identify the chance of certain event happens.

4. A Gaussian with mean $\mu$ and variance $\sigma^2$ can be written as a simple transformation of the standard Gaussian with mean $0.$ and variance $1.$.

*  [[1pts]] Write the transformation that takes $x \sim \mathcal{N}(0.,1.)$ to $z \sim \mathcal{N}(\mu, \sigma^2)$:

Answer:
Let $X$ be a continuous random variable following a standard Gaussian distribution with $\mu = 0.$ and $\sigma = 1.$.
Let $Z$ be a continuous random variable, where $Z = \mu + \sigma X$. And, $Z$ is also normally distributed with mean μ and variance $\sigma^2$.

*  [[2pts]] Write a code implementation to produce $n$ independent samples from $\mathcal{N}(\mu, \sigma^2)$ by transforming $n$ samples from $\mathcal{N}(0.,1.)$.

Answer:

```julia;
    function sample_gaussian(n; mean=0., variance=0.01)
      # n samples from standard gaussian
      x = randn(n)

      # transform x to sample z from N(mean,variance)
      z = mean .+ sqrt(variance) .* x
      return z
    end;
```

[2pts] Test your implementation by computing statistics on the samples:

```julia
    using Test
    using Statistics: mean, var
    @testset "Numerically testing Gaussian Sample Statistics" begin
      #TODO: Sample 100000 samples with your function and use mean and var to
      # compute statistics.
      # tests should compare statistics against the true mean and variance from arguments.
      # hint: use isapprox with keyword argument atol=1e-2
      n = 100000
      @test isapprox(mean(sample_gaussian(n,mean=10., variance=1)) , 10 ,atol=1e-2)
      @test isapprox(var(sample_gaussian(n,mean=10., variance=1)) , 1 ,atol=1e-2)

    end;
```

5. [3pts] Sample $10000$ samples from a Gaussian with mean $10.$ an variance $2$. Plot the **normalized** `histogram` of these samples. On the same axes `plot!` the pdf of this distribution.
Confirm that the histogram approximates the pdf.
(Note: with `Plots.jl` the function `plot!` will add to the existing axes.)

```julia; fig_cap="Figure: Histogram of a Gaussian distribution"
    using Plots
    using Distributions
    n=10000
    mu=10
    sigma2=2
    x = mu .+ sqrt(sigma2) .* randn(n)
    nd=Normal(mu,sigma2)
    #histogram
    histogram(x,normalize=true, label="normal histogram")
    #plot!
    plot!(x->pdf.(nd,x),
        label = "normal pdf",
        xlabel="x",
        ylabel="f(x)",
        title="Histogram of a Gaussian distribution")
```


# Calculus

## Manual Differentiation

Let $x,y \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, and square matrix $B \in \mathbb{R}^{m \times m}$.
And where $x'$ is the transpose of $x$.
Answer the following questions in vector notation.

1. [1pts] What is the gradient of $x'y$ with respect to $x$?

Answer:
$\frac{\partial{(x'y)}}{\partial{x}} = \frac{\partial{(x_{1}y_{1}\ +\ \dots \ + \ x_{m}y_{m})}}{\partial{(x_{1},\ \dots,\ x_{m})}} = y$

2. [1pts] What is the gradient of $x'x$ with respect to $x$?

Answer:
$\frac{d(x'x)}{\partial{x}} = \frac{d(x_{1}^2\ +\ \dots \ + \ x_{m}^2)}{\partial{(x_{1},\ \dots,\ x_{m})}} = 2x$

3. [2pts] What is the Jacobian of $x'A$ with respect to $x$?

Answer:
Let $x'$ be $[x_{1}\ x_{2}\ \dots \ x_{m}]$ a $1 \times m$ vector.
Let A be a $m \times n$ matrix, i.e.
$$
\left(\begin{array}{cc}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{array}\right)
$$
Thus, $x'A$ is
$$
\left(\begin{array}{cc}
a_{11}x_{1}\ +\ \dots \ +\ a_{m1}x_{m} & a_{12}x_{1}\ +\ \dots \ +\ a_{m2}x_{m}\ & \dots & a_{1n}x_{1}\ +\ \dots \ +\ a_{mn}x_{m}
\end{array}\right)
$$
And, Jacobian of $x'A$ with respect to $x$ can be written a $n \times m$ matrix
$$
\left(\begin{array}{cc}
a_{11} & a_{21} & \dots & a_{m1} \\
a_{12} & a_{22} & \dots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \dots & a_{mn}
\end{array}\right)
$$
which is $\mathbf{A}'$.

4. [2pts] What is the gradient of $x'Bx$ with respect to $x$?

Answer:
Let $y = Bx$, then
$$
\begin{align}
\frac{d(x'Bx)}{d{x}} &= \frac{d(x'y)}{d{x}}\\
&= \frac{\partial{(x'y)}}{\partial{x}} + \frac{d(y'(x))}{\partial{x}} * \frac{\partial{(x'y)}}{\partial{y}}\\
&= y + B'x\\
&= (B + B')x
\end{align}
$$


## Automatic Differentiation (AD)

Use one of the accepted AD library (Zygote.jl (julia), JAX (python), PyTorch (python)) to implement and test your answers above.

### [1pts] Create Toy Data

```julia
    # Choose dimensions of toy data
    m = 5
    n = 3

    # Make random toy data with correct dimensions
    x = rand(m)
    y = rand(m)
    A = rand(m,n)
    B = rand(m,m)
```

[1pts] Test to confirm that the sizes of your data is what you expect:

```julia
    using Test
    # Make sure your toy data is the size you expect!
    @testset "Sizes of Toy Data" begin
      # confirm sizes for toy data x,y,A,B
      #hint: use `size` function, which returns tuple of integers.
      @test size(x) == (m,)
      @test size(y) == (m,)
      @test size(A) == (m,n)
      @test size(B) == (m,m)
    end;
```


### Automatic Differentiation

1. [1pts] Compute the gradient of $f_1(x) = x'y$ with respect to $x$?

```julia
    # Use AD Tool
    using Zygote: gradient
    # note: `Zygote.gradient` returns a tuple of gradients, one for each argument.
    # if you want just the first element you will need to index into the tuple with [1]

    f1(x,y) = transpose(x) * y
    df1dx = gradient((x,y)->f1(x,y), x, y)
```

2. [1pts] Compute the gradient of $f_2(x) = x'x$ with respect to $x$?

```julia
    f2(x) = transpose(x) * x
    df2dx = gradient(x->f2(x), x)
```

3. [1pts] Compute the Jacobian of $f_3(x) = x'A$ with respect to $x$?
If you try the usual `gradient` fucntion to compute the whole Jacobian it would give an error.
You can use the following code to compute the Jacobian instead.

```julia
    function jacobian(f, x)
        y = f(x)
        n = length(y)
        m = length(x)
        T = eltype(y)
        j = Array{T, 2}(undef, n, m)
        for i in 1:n
            j[i, :] .= gradient(x -> f(x)[i], x)[1]
        end
        return j
    end

    f3(x) = transpose(x) * A
    df3dx = jacobian(f3,x)
    # use jacobian
```

[2pts] Briefly, explain why `gradient` of $f_3$ is not well defined (hint: what is the dimensionality of the output?) and what the `jacobian` function is doing in terms of calls to `gradient`.
Specifically, how many calls of `gradient` is required to compute a whole `jacobian` for $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$?

Answer:

The very important takeaway here is that, with AD, `gradient`s are cheap but full `jacobian`s are expensive.

`gradient` of $f_3$ is not well defined, becase the dimensionality of the output is a $n \times m$ matrix, not a scalar.
In function `jacobian`, to calculate `gradient` of $f_3$, a loop is created to take each term of $f_3$ out and calculate `gradient` with respect to $x$ repectively.
The resulting array forms the rows of a pre-defined $n \times m$ matrix. After the iteration, the Jacobian of $f_3(x) = x'A$ with respect to $x$ is output.
Thus, the number of calls of `gradient` is equal the dimentions $n$ of $f_3$, which is 3 in this case.

4. [1pts] Compute the gradient of $f_4(x) = x'Bx$ with respect to $x$?

```julia
    f4(x) = transpose(x) * B * x
    df4dx = gradient(x->f4(x),x)
```

5. [2pts] Test all your implementations against the manually derived derivatives in previous question

```julia
    using Test
    # Test to confirm that AD matches hand-derived gradients
    @testset "AD matches hand-derived gradients" begin
      @test df1dx[1] == y # rhs from 2.1
      @test df2dx[1] == 2 .* x # rhs from 2.1
      @test df3dx == transpose(A) # rhs from 2.1
      @test df4dx[1] ≈ (B + transpose(B)) * x # rhs from 2.1
    end
```

```julia; eval=false
using Weave
weave(joinpath(dirname(pwd()),"Assignment0","A0.jmd"), doctype = "md2pdf", out_path=:pwd())
```
